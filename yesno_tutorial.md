# CATï¼ˆyesnoï¼‰é¡¹ç›®æ­å»ºæµç¨‹

**ç›®å½•**
* [é¡¹ç›®ç›®å½•ç»“æ„](#é¡¹ç›®ç›®å½•ç»“æ„)
* [0.æ–‡ä»¶å‡†å¤‡](#0-æ–‡ä»¶å‡†å¤‡)
* [1.æ•°æ®å‡†å¤‡](#1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87)
* 	* [prepare_data.sh](#prepare_datash)
	* [prepare_dict.sh](#prepare_dictsh)
	* [T.fst & L.fst](#tfst--lfst)
	* [G.fst](#gfst)
	* [TLG.fst](#tlgfst)
* [2.æå–FBankç‰¹å¾](#2-%E6%8F%90%E5%8F%96fbank%E7%89%B9%E5%BE%81)
* [3.å‡†å¤‡åˆ†æ¯å›¾è¯­è¨€æ¨¡å‹](#3-%E5%87%86%E5%A4%87%E5%88%86%E6%AF%8D%E5%9B%BE%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)
* [4.ç¥ç»ç½‘ç»œè®­ç»ƒå‡†å¤‡](#4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87)
* [5.æ¨¡å‹è®­ç»ƒ](#5-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83)
* [6.è§£ç ](#6-%E8%A7%A3%E7%A0%81)
* [7.å®éªŒå±•ç¤º](#7-å®éªŒå±•ç¤º)
* [8.ç»“æœåˆ†æ](#8-ç»“æœåˆ†æ)

æ­¤æ–‡æ¡£çš„ç›®çš„æ˜¯è®©å¤§å®¶äº†è§£kaldiå·¥å…·åŒ…çš„ä½¿ç”¨ï¼Œ**é€šè¿‡æ­å»ºä¸€ä¸ªç®€å•çš„è¯­éŸ³è¯†åˆ«é¡¹ç›®ï¼Œå¸®åŠ©åˆå­¦è€…æ›´å¤šäº†è§£CATçš„å·¥ä½œæµç¨‹ï¼Œå…ˆçŸ¥å…¶ç„¶ï¼Œåœ¨çŸ¥å…¶æ‰€ä»¥ç„¶ï¼Œå¦‚æœæƒ³è¦æ›´å¤šäº†è§£å»ºè®®è¿›ä¸€æ­¥é˜…è¯»ä»¥ä¸‹åŸºæœ¬æ–‡çŒ®ã€‚

- L. R. Rabiner, â€œA tutorial on hidden Markov models and selected applications in speech recognitionâ€, Proceedings of the IEEE, 1989.[PDF](https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf)
- A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, â€œConnectionist temporal classiï¬cation: Labelling unsegmented sequence data with recurrent neural networksâ€, ICML, 2006.[PDF](https://www.cs.toronto.edu/~graves/icml_2006.pdf)
- Hongyu Xiang, Zhijian Ou, "CRF-based Single-stage Acoustic Modeling with CTC Topology", ICASSP, 2019.[PDF](http://oa.ee.tsinghua.edu.cn/~ouzhijian/pdf/ctc-crf.pdf)
- Zhijian Ou, "State-of-the-Art of End-to-End Speech Recognition", Tutorial at The 6th Asian Conference on Pattern Recognition (ACPR2021), Jeju Island, Korea, 2021.[PDF](http://oa.ee.tsinghua.edu.cn/~ouzhijian/pdf/ACPR2021%20Tutorial%20State-of-the-Art%20of%20End-to-End%20Speech%20Recognition.pdf)

**[CAT workflow](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md)å·²ç»æ•´ç†äº†CATçš„å·¥ä½œæµç¨‹ï¼Œåˆ†ä¸ºå…­æ­¥ï¼Œå‰äº”æ­¥ä¸ºè®­ç»ƒï¼Œç¬¬å…­æ­¥æ˜¯è§£ç ã€‚** è¿™ä»½æ–‡æ¡£å°†æ ¹æ®[CAT workflow](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md)ï¼Œæ›´å…·ä½“åœ°ä»¥ä¸€ä¸ªç®€å•è¯­éŸ³è¯†åˆ«é¡¹ç›®ï¼ˆyesnoé¡¹ç›®ï¼‰ä¸ºä¾‹ï¼Œå¯¹CATå·¥ä½œæµç¨‹åŠ ä»¥è§£é‡Šã€‚

yesnoè¯­éŸ³è¯†åˆ«é¡¹ç›®ï¼Œæ¥è‡ª[Kaldiä¸­çš„yesnoé¡¹ç›®](https://github.com/kaldi-asr/kaldi/tree/master/egs/yesno)ã€‚å¦‚ä¸‹æ‰€è¿°ï¼Œyesnoé¡¹ç›®åªå«æœ‰ä¸¤ä¸ªè¯æ±‡ï¼Œyeså’Œnoï¼›ä¸€å¥è¯ä¸­ä¼šåŒ…å«å¤šä¸ªç”±å¸Œä¼¯æ¥è¯­ï¼ˆHebrewï¼‰è¯´çš„yeså’Œnoã€‚

```
The "yesno" corpus is a very small dataset of recordings of one individual
saying yes or no multiple times per recording, in Hebrew.  It is available from
http://www.openslr.org/1.
```

## é¡¹ç›®ç›®å½•ç»“æ„

**ä¸€ä¸ªè¯­éŸ³è¯†åˆ«é¡¹ç›®**ï¼ŒæŒ‡åœ¨ä¸€ä¸ªç‰¹å®šçš„æ•°æ®é›†ä¸Šçš„é¡¹ç›®ï¼Œé€šå¸¸å„ä¸ªé¡¹ç›®åœ¨egsæ–‡ä»¶ç›®å½•ä¸‹ï¼Œä¹Ÿå¯ä»¥å°è¯•è®­ç»ƒegsç›®å½•ä¸‹çš„å…¶å®ƒæ•°æ®é›†å®éªŒã€‚

**yesno**

```
â”œâ”€â”€ cmd.sh #è„šæœ¬é…ç½®
â”œâ”€â”€ path.sh #ç¯å¢ƒå˜é‡é…ç½®
â”œâ”€â”€ run.sh #å®éªŒä¸»ç¨‹åº
â”œâ”€â”€ conf #é…ç½®æ–‡ä»¶ç›®å½•
â”‚Â Â  â”œâ”€â”€ decode_dnn.config #è§£ç 
â”‚   â”œâ”€â”€ fbank.conf #fbankæå–
â”‚   â””â”€â”€ mfcc.conf #mfccæå–
â”œâ”€â”€ ctc-crf -> ../../scripts/ctc-crf #ctc-crfç¨‹åº
â”œâ”€â”€ exp #æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ demo #demoæ¨¡å‹
â”‚   â”‚   â””â”€â”€ config.json #demoæ¨¡å‹çš„è®­ç»ƒå‚æ•°
â”œâ”€â”€ input #è¾“å…¥ç›®å½•
â”‚   â””â”€â”€ lexicon.txt #yesnoå­—å…¸
â”œâ”€â”€ local #å­˜æ”¾ä¸»ç¨‹åºè¿è¡Œå„éƒ¨åˆ†è„šæœ¬å—
â”‚   â”œâ”€â”€ create_yesno_txt.pl #æ•°æ®é¢„å¤„ç†waves.txt(éŸ³é¢‘IDå’Œå¯¹åº”æœ¬åœ°è·¯å¾„)
â”‚   â”œâ”€â”€ create_yesno_waves_test_train.pl #æ•°æ®è®­ç»ƒå¼€å‘é›†åˆ’åˆ†
â”‚   â”œâ”€â”€ create_yesno_wav_scp.pl #æ•°æ®é¢„å¤„ç†waves.scpï¼ˆéŸ³é¢‘IDå’Œå¯¹åº”éŸ³é¢‘å†…å®¹ï¼‰
â”‚   â”œâ”€â”€ get_word_map.pl #å¯¹æ¯ä¸ªè¯å»ºç«‹æ˜ å°„
â”‚Â Â  â”œâ”€â”€ prepare_data.sh #æ•°æ®é¢„å¤„ç†ç¨‹åº
â”‚Â Â  â”œâ”€â”€ prepare_dict.sh #è¯å…¸é¢„å¤„ç†ç¨‹åº
â”‚   â”œâ”€â”€ score.sh #æ‰“åˆ†è„šæœ¬ï¼ˆWERï¼‰
â”‚Â Â  â”œâ”€â”€ yesno_decode_graph.sh #fstæ–‡ä»¶æ•´ç†æ‰“åŒ…
â”‚   â””â”€â”€ yesno_train_lms.sh #è¯­è¨€æ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ steps -> /myhome/kaldi/egs/wsj/s5/steps #é“¾æ¥åˆ°kaldiä¸­åŒåç›®å½•ï¼ŒåŒ…å«å„ä¸ªè®­ç»ƒé˜¶æ®µçš„å­è„šæœ¬ï¼Œå¦‚ç‰¹å¾æå– make_fbank.shç­‰ï¼Œæ­¤è·¯å¾„è½¯è¿æ¥åˆ°Kaldiæ‰€åœ¨è·¯å¾„
â””â”€â”€ utils -> /myhome/kaldi/egs/wsj/s5/utils #é“¾æ¥åˆ°kaldiä¸­åŒåç›®å½•ï¼Œç”¨äºååŠ©å¤„ç†ï¼Œå¦‚æ•°æ®å¤åˆ¶ä¸éªŒè¯ç­‰
```

æ¥ä¸‹æ¥æˆ‘ä»¬å°†åˆ©ç”¨CATå’Œyesnoæ•°æ®ï¼Œä¸€æ­¥æ­¥æ­å»ºä¸€ä¸ªè¯­éŸ³è¯†åˆ«é¡¹ç›®ï¼Œå†æ¬¡ä¹‹å‰è¯·ç¡®ä¿æ‚¨å·²ç»å®Œæˆäº†[CATç¯å¢ƒé…ç½®](https://github.com/HPLQAQ/CAT-tutorial/blob/master/environment.md)å’Œ[CATçš„å®‰è£…](https://github.com/thu-spmi/CAT#Installation)ã€‚


## 0. æ–‡ä»¶å‡†å¤‡

åœ¨è¿™éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å…ˆå‡†å¤‡å¥½é¡¹ç›®æ‰€éœ€è¦çš„æ•´ä½“æ¡†æ¶ã€‚

1. åœ¨egsä¸‹åˆ›å»ºyesnoç›®å½•

2. ç¼–å†™ä»¥ä¸‹ä¸¤ä¸ªè„šæœ¬
CAT toolkit: ä¸€èˆ¬æ— éœ€ä¿®æ”¹é»˜è®¤è·¯å¾„å³å¯
Kaldi:è·¯å¾„éœ€è¦ä¿®æ”¹åˆ°ä¸‹è½½å¥½çš„kaldiæ ¹ç›®å½•ä¸‹
Data:ä½ çš„yesnoæ ¹ç›®å½•ä¸‹
   - **path.sh**

     ```shell
     # CAT toolkit
     export CAT_ROOT=../../
     export PATH=$CAT_ROOT/src/ctc_crf/path_weight/build:$PATH
     export PATH=$PWD/ctc-crf:$PATH
     # Kaldi
     export KALDI_ROOT=${KALDI_ROOT:-/myhome/kaldi}
     [ -f $KALDI_ROOT/tools/env.sh ] && . $KALDI_ROOT/tools/env.sh
     export PATH=$PWD/utils/:$KALDI_ROOT/tools/openfst/bin:$PWD:$PATH
     [ ! -f $KALDI_ROOT/tools/config/common_path.sh ] && echo >&2 "The standard file $KALDI_ROOT/tools/config/common_path.sh is not present -> Exit!" && exit 1
     . $KALDI_ROOT/tools/config/common_path.sh
     export LC_ALL=C
     # Data
     export DATA_ROOT=data/yesno
     ```

     é…ç½®å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œåˆ†åˆ«é…ç½®CATã€kaldiã€Data(æ•°æ®é›†çš„ç¯å¢ƒå˜é‡)ï¼Œä»£ç æ¥æºä¸º`egs\wsj`é¡¹ç›®ä¸‹çš„åŒåæ–‡ä»¶ã€‚

     åˆ›å»ºå®Œåå¯ä»¥åœ¨ç»ˆç«¯é‡Œè¿è¡Œä¸€é`./path.sh`ï¼Œæ²¡æœ‰é—®é¢˜åæˆ‘ä»¬è¿›è¡Œä¸‹ä¸€æ­¥ã€‚

   - **cmd.sh**

     ```shell
     export train_cmd=run.pl
     export decode_cmd=run.pl
     export mkgraph_cmd=run.pl
     export cuda_cmd=run.pl
     ```

     è¿™é‡Œæ˜¯æ²¿ç”¨æ¥è‡ªkaldiçš„å¹¶è¡ŒåŒ–å·¥å…·ï¼Œé€‚åº”ä¸åŒçš„ç¯å¢ƒå¯ä»¥é…ç½®queue.plç­‰ä»¥åŠä¸åŒçš„å‚æ•°ã€‚ä¸€èˆ¬æƒ…å†µä¸‹æˆ‘ä»¬é»˜è®¤run.plå³å¯ã€‚

3. åˆ›è½¯è¿æ¥åˆ°kaldiä»¥åŠCATå·¥å…·åŒ…çš„ç›®å½•ï¼Œä¾¿äºä»£ç çš„ç¼–å†™ä»¥åŠè¿ç§»

   ```shell
   ln -s ../../scripts/ctc-crf ctc-crf
   ln -s $KALDI_ROOT/egs/wsj/s5/utils utils
   ln -s $KALDI_ROOT/egs/wsj/s5/steps steps
   ```

4. åˆ›å»ºlocalç›®å½•ï¼Œå­˜æ”¾æœ¬é¡¹ç›®ä¸“ç”¨æ•°æ®é›†ï¼Œè®­ç»ƒï¼Œåˆ‡åˆ†ï¼Œæ‰“åˆ†ç­‰è„šæœ¬ç¼–å†™

5. åˆ›å»º**run.sh**ï¼Œæˆ‘ä»¬åœ¨run.shå®Œæˆæ•´ä½“ç¼–å†™

   ```shell
   #!/bin/bash
   
   # Copyright 2022 TasiTech
   # Author: Ziwei Li
   # yesno for CAT
   
   # environment
   . ./cmd.sh
   . ./path.sh
   
   #set 
   H=`pwd`  # home dir
   n=12     # parallel jobs=$(nproc)
   stage=0  # set work stages
   stop_stage=9
   change_config=0
   yesno=$DATA_ROOT  #data root
   
   . utils/parse_options.sh
   
   NODE=$1
   if [ ! $NODE ]; then
     NODE=0
   fi
   
   if [ $NODE == 0 ]; then
     if [ $stage -le 1 ] && [ $stop_stage -ge 1 ]; then
       echo "stage 1: *"
       # work
     fi
   
     #more stages
   fi
   ```
   
   $NODEæŒ‡å®éªŒè¿è¡Œçš„èŠ‚ç‚¹æ•°ï¼Œè‹¥è¿è¡Œrun.shæ—¶ç›´æ¥ä¼ å‚èŠ‚ç‚¹æ•°ï¼Œç”¨stageå’Œstop_stageæ§åˆ¶ä»£ç è¿è¡Œéƒ¨åˆ†ã€‚

## 1. æ•°æ®å‡†å¤‡
Step 1: [Data preparation](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md#Data-preparation)

æˆ‘ä»¬å®Œæˆäº†æ¡†æ¶å‡†å¤‡è‡ªæ­¤è¿›å…¥[CAT workflow](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md)çš„å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬æŒ‰é¡ºåºç¼–å†™æ¯ä¸ªè„šæœ¬ã€‚

åœ¨run.shä¸­step 1ï¼Œæˆ‘ä»¬å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼šè·å–è®­ç»ƒæ•°æ®ï¼Œå»ºç«‹æ‰€éœ€å­—å…¸ï¼Œè®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚

ä»¥ä¸‹ä¸ºstep 1çš„ä»£ç ï¼Œåœ¨æœ¬èŠ‚ä¸­æˆ‘ä»¬ä¼šè¯¦ç»†è§£é‡Šè¿™éƒ¨åˆ†ä»£ç çš„æ€è·¯ã€‚

```shell
if [ $stage -le 1 ] && [ $stop_stage -ge 1 ]; then
  echo "stage 1: Data Preparation and FST Construction"

  local/prepare_data.sh || exit 1; # Get data and lists
  local/prepare_dict.sh || exit 1; # Get lexicon dict

  # Compile the lexicon and token FSTs
  # generate lexicon FST L.fst according to words.txt, generate token FST T.fst according to tokens.txt
  ctc-crf/ctc_compile_dict_token.sh --dict-type "phn" \
    data/dict data/local/lang_phn_tmp data/lang || exit 1;
  
  # Train and compile LMs. Generate G.fst according to lm, and compose FSTs into TLG.fst
  local/yesno_train_lms.sh data/train/text data/dict/lexicon.txt data/lm || exit 1;
  local/yesno_decode_graph.sh data/lm/srilm/srilm.o1g.kn.gz data/lang data/lang_test || exit 1;
fi
```

### prepare_data.sh

æˆ‘ä»¬å°†æ•°æ®ä¸‹è½½å‡†å¤‡çš„æ­¥éª¤æ”¾åœ¨prepare_data.shä¸­å®Œæˆã€‚åœ¨prepare.shå®Œæˆåï¼Œæˆ‘ä»¬æœŸæœ›è·å¾—ä»¥åŠåˆ’åˆ†ä¸ºè®­ç»ƒé›†(train)ä¸å¼€å‘é›†(dev)çš„dataï¼ˆwav.scpï¼‰ï¼Œè¯´è¯äººä¿¡æ¯ï¼ˆspk2uttã€utt2spkï¼Œè¿™é‡Œè¯´è¯äººæˆ‘ä»¬é»˜è®¤ä»–ä¸ºglobalï¼‰ï¼Œæ ‡æ³¨æ–‡æœ¬ä¿¡æ¯ï¼ˆtextï¼‰ï¼Œåˆ†åˆ«å­˜å‚¨åœ¨data/dev,data/trainä¸‹ã€‚

1. åœ¨localç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶prepare_data.shï¼Œå¹¶è·å–æ•°æ®

   ```shell
   #!/usr/bin/env bash
   # This script prepares data and create necessary files
   
   . ./path.sh
   
   data=${H}/data
   local=${H}/local 
   mkdir -p ${data}/local
   
   cd ${data}
   
   # acquire data if not downloaded
   if [ ! -d waves_yesno ]; then
     echo "Getting Data"
     wget http://www.openslr.org/resources/1/waves_yesno.tar.gz || exit 1;
     tar -xvzf waves_yesno.tar.gz || exit 1;
     rm waves_yesno.tar.gz || exit 1;
   fi
   ```

   è¿™ä¸€æ­¥å®Œæˆåï¼Œæˆ‘ä»¬åœ¨data/waves_yesnoä¸‹å¾—åˆ°åŸå§‹éŸ³é¢‘æ•°æ®é›†ã€‚

2. ç”±äºæ•°æ®ä¸”æ²¡æœ‰åˆ’åˆ†ï¼Œè¿™éƒ¨åˆ†æˆ‘ä»¬å°†éŸ³é¢‘æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†(train)å’Œå¼€å‘é›†(dev)

   æ³¨ï¼šç”±äºæ•°æ®é‡è¾ƒå°è¿™é‡Œç›´æ¥å°†å¼€å‘é›†ä½œä¸ºæµ‹è¯•é›†ï¼Œå¯ä»¥ä¿®æ”¹

   ```shell
   echo "Preparing train and dev data"
   
   rm -rf train dev
   
   # Create waves list and Divide into dev and train set
   waves_dir=${data}/waves_yesno
   ls -1 $waves_dir | grep "wav" > ${data}/local/waves_all.list
   cd ${data}/local
   ${local}/create_yesno_waves_test_train.pl waves_all.list waves.dev waves.train
   ```
  
  æˆ‘ä»¬å®Œæˆåç”Ÿæˆcreate_yesno_waves_test_train.plåæˆ‘ä»¬å¯¹å…¶è¿›è¡Œç¼–å†™
   
   **create_yesno_waves_test_train.pl**

   æ³¨ï¼šè¿™éƒ¨åˆ†ä»£ç æ¥æºäºkaldiä¸­yesnoé¡¹ç›®

   .plä¸ºperlä»£ç ï¼Œæ­¤éƒ¨åˆ†ä»£ç æ¯”è¾ƒéš¾ç†è§£ã€‚

   ```perl
   #!/usr/bin/env perl
   
   $full_list = $ARGV[0];
   $test_list = $ARGV[1];
   $train_list = $ARGV[2];
   
   open FL, $full_list;
   $nol = 0;
   while ($l = <FL>)
   {
   	$nol++;
   }
   close FL;
   
   $i = 0;
   open FL, $full_list;
   open TESTLIST, ">$test_list";
   open TRAINLIST, ">$train_list";
   while ($l = <FL>)
   {
   	chomp($l);
   	$i++;
   	if ($i <= $nol/2 )
   	{
   		print TRAINLIST "$l\n";
   	}
   	else
   	{
   		print TESTLIST "$l\n";
   	}
   }
   ```

   ç­‰åˆ†waves_all.liståˆ°waves.dev, waves.trainä¸­

æˆ‘ä»¬ç»§ç»­å›åˆ°prepare.data.shç”Ÿæˆtest.txtå’Œwave.scp

3. ç”Ÿæˆ\*_wav.scp, \*.txt(\*ä»£æŒ‡train, test, dev)

   ```shell
   cd ${data}/local
   
   for x in train dev; do
     # create id lists
     ${local}/create_yesno_wav_scp.pl ${waves_dir} waves.$x > ${x}_wav.scp #id to wavfile
     ${local}/create_yesno_txt.pl waves.$x > ${x}.txt #id to content
   done
   
   ${local}/create_yesno_wav_scp.pl ${waves_dir} waves.dev > test_wav.scp #id to wavfile
   ${local}/create_yesno_txt.pl waves.dev > test.txt #id to content
   ```

ç”Ÿæˆ*.scpæ–‡ä»¶æ ¼å¼ä¸ºéŸ³é¢‘IDå’Œå¯¹åº”çš„å­˜å‚¨ä½ç½®
  
  **create_yesno_wav_scp.pl**

   åˆ›å»º*.scpæ–‡ä»¶ï¼Œå†…å®¹ä¸ºæ–‡ä»¶åå¯¹åº”çš„å­˜å‚¨ä½ç½®ã€‚

   ```perl
   #!/usr/bin/env perl
   
   $waves_dir = $ARGV[0];
   $in_list = $ARGV[1];
   
   open IL, $in_list;
   
   while ($l = <IL>)
   {
   	chomp($l);
   	$full_path = $waves_dir . "\/" . $l;
   	$l =~ s/\.wav//;
   	print "$l $full_path\n";
   }
   ```
ç”Ÿæˆ*.txtæ–‡ä»¶ï¼Œæœªè§å†…å®¹ä¸ºéŸ³é¢‘IDå’Œå¯¹åº”çš„æ–‡æœ¬å†…å®¹


   **create_yesno_txt.pl**

   åˆ›å»º.txtæ–‡ä»¶ï¼Œå†…å®¹ä¸ºæ–‡ä»¶åå¯¹åº”çš„æ–‡æœ¬å†…å®¹ã€‚

   ```perl
   #!/usr/bin/env perl
   
   $in_list = $ARGV[0];
   
   open IL, $in_list;
   
   while ($l = <IL>)
   {
   	chomp($l);
   	$l =~ s/\.wav//;
   	$trans = $l;
   	$trans =~ s/0/NO/g;
   	$trans =~ s/1/YES/g;
   	$trans =~ s/\_/ /g;
   	print "$l $trans\n";
   }
   ```

æœ€åç¼–å†™prepare_data.shç”Ÿæˆutt2spkï¼Œspk2utt
   

4. å°†æ•°æ®è½¬ç§»åˆ°data/dev, data/train, data/testä¸‹ï¼Œå¹¶ç”Ÿæˆutt2spk, spk2utt

   ```shell
   for x in train dev test; do
     # sort wave lists and create utt2spk, spk2utt
     mkdir -p $x
     sort local/${x}_wav.scp -o $x/wav.scp
     sort local/$x.txt -o $x/text
     cat $x/text | awk '{printf("%s global\n", $1);}' > $x/utt2spk
     sort $x/utt2spk -o $x/utt2spk
     ${H}/utils/utt2spk_to_spk2utt.pl < $x/utt2spk > $x/spk2utt
   done
   ```

   utilså’Œstepç›®å½•ä¸‹çš„è„šæœ¬å‡ä¸ºkaldiçš„è„šæœ¬ï¼Œåœ¨å…¶ç›®å½•ä¸‹æœ‰è¯¦ç»†è§£é‡Šã€‚

   è¿™ä¸€æµç¨‹å®Œæˆåï¼Œdataä¸‹çš„ç›®å½•ç»“æ„ä¸ºï¼š
   
   ```
   â”œâ”€â”€ dev #å¼€å‘é›†
   â”‚   â”œâ”€â”€ spk2utt #è¯´è¯äºº-éŸ³é¢‘ID
   â”‚   â”œâ”€â”€ text #éŸ³é¢‘ID-æ–‡æœ¬
   â”‚   â”œâ”€â”€ utt2spk #éŸ³é¢‘ID-è¯´è¯äºº
   â”‚   â””â”€â”€ wav.scp #éŸ³é¢‘ID-æ–‡ä»¶ä½ç½®
   â”œâ”€â”€ train #è®­ç»ƒé›†
   â”‚   â”œâ”€â”€ spk2utt
   â”‚   â”œâ”€â”€ text
   â”‚   â”œâ”€â”€ utt2spk
   â”‚   â””â”€â”€ wav.scp
   â”œâ”€â”€ test #æµ‹è¯•é›†
   â”‚   â”œâ”€â”€ spk2utt
   â”‚   â”œâ”€â”€ text
   â”‚   â”œâ”€â”€ utt2spk
   â”‚   â””â”€â”€ wav.scp
   â”œâ”€â”€ local #ä¸­é—´æ–‡ä»¶
   â”‚   â”œâ”€â”€ dev.txt #å¼€å‘é›†çš„text
   â”‚   â”œâ”€â”€ dev_wav.scp #å¼€å‘é›†çš„wav.scp
   â”‚   â”œâ”€â”€ test.txt
   â”‚   â”œâ”€â”€ test_wav.scp
   â”‚   â”œâ”€â”€ train.txt
   â”‚   â”œâ”€â”€ train_wav.scp
   â”‚   â”œâ”€â”€ waves.dev 
   â”‚   â”œâ”€â”€ waves.train #è®­ç»ƒé›†æ–‡ä»¶ååˆ—è¡¨
   â”‚   â””â”€â”€ waves_all.list
   â””â”€â”€ waves_yesno #éŸ³é¢‘æ•°æ®é›†å­˜å‚¨ä½ç½®
   ```

   ä»¥ä¸‹å±•ç¤ºtrainç›®å½•ä¸‹çš„æ–‡ä»¶çš„éƒ¨åˆ†å†…å®¹ï¼š

   **spk2utt**
   
   [speaker] [wav_name1] [wav_name2] ...

   ```
   global 0_0_0_0_1_1_1_1 0_0_0_1_0_0_0_1 0_0_0_1_0_1_1_0 0_0_1_0_0_0_1_0 0_0_1_0_0_1_1_0 0_0_1_0_0_1_1_1 0_0_1_0_1_0_0_0 0_0_1_0_1_0_0_1 0_0_1_0_1_0_1_1 0_0_1_1_0_0_0_1 0_0_1_1_0_1_0_0 0_0_1_1_0_1_1_0 0_0_1_1_0_1_1_1 0_0_1_1_1_0_0_0 0_0_1_1_1_0_0_1 0_0_1_1_1_1_0_0 0_0_1_1_1_1_1_0 0_1_0_0_0_1_0_0 0_1_0_0_0_1_1_0 0_1_0_0_1_0_1_0 0_1_0_0_1_0_1_1 0_1_0_1_0_0_0_0 0_1_0_1_1_0_1_0 0_1_0_1_1_1_0_0 0_1_1_0_0_1_1_0 0_1_1_0_0_1_1_1 0_1_1_1_0_0_0_0 0_1_1_1_0_0_1_0 0_1_1_1_0_1_0_1 0_1_1_1_1_0_1_0
   ```
   
   **utt2spk**
   
   [wav_name] [speaker]
   
   ```
   0_0_0_0_1_1_1_1 global
   0_0_0_1_0_0_0_1 global
   0_0_0_1_0_1_1_0 global
   0_0_1_0_0_0_1_0 global
   0_0_1_0_0_1_1_0 global
   ...
   ```
   
   **wav.scp**
   
   [wav_name] [wav_location]

   ```
   0_0_0_0_1_1_1_1 /myhome/CAT/egs/yesno/data/waves_yesno/0_0_0_0_1_1_1_1.wav
   0_0_0_1_0_0_0_1 /myhome/CAT/egs/yesno/data/waves_yesno/0_0_0_1_0_0_0_1.wav
   0_0_0_1_0_1_1_0 /myhome/CAT/egs/yesno/data/waves_yesno/0_0_0_1_0_1_1_0.wav
   0_0_1_0_0_0_1_0 /myhome/CAT/egs/yesno/data/waves_yesno/0_0_1_0_0_0_1_0.wav
   ...
   ```
   
   **text**
   
   [wav_name] [wav_content]
   
   ```
   0_0_0_0_1_1_1_1 NO NO NO NO YES YES YES YES
   0_0_0_1_0_0_0_1 NO NO NO YES NO NO NO YES
   0_0_0_1_0_1_1_0 NO NO NO YES NO YES YES NO
   0_0_1_0_0_0_1_0 NO NO YES NO NO NO YES NO
   0_0_1_0_0_1_1_0 NO NO YES NO NO YES YES NO
   ...
   ```
   
   é€šè¿‡ç”Ÿæˆè¿™äº›å›ºå®šæ ¼å¼çš„æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨kaldiçš„å·¥å…·æ¥ä¼˜åŒ–å·¥ä½œæµç¨‹ã€‚
   
   å½“å‰ç›®å½•ä¸‹ï¼š
   
   ```
   â”œâ”€â”€ cmd.sh
   â”œâ”€â”€ ctc-crf -> ../../scripts/ctc-crf
   â”œâ”€â”€ data
   â”‚   â”œâ”€â”€ dev
   â”‚   â”œâ”€â”€ local
   â”‚   â”œâ”€â”€ test
   â”‚   â”œâ”€â”€ train
   â”‚   â””â”€â”€ waves_yesno
   â”œâ”€â”€ local
   â”‚   â”œâ”€â”€ create_yesno_txt.pl
   â”‚   â”œâ”€â”€ create_yesno_wav_scp.pl
   â”‚   â”œâ”€â”€ create_yesno_waves_test_train.pl
   â”‚   â””â”€â”€ prepare_data.sh
   â”œâ”€â”€ path.sh
   â”œâ”€â”€ run.sh
   â”œâ”€â”€ steps -> /myhome/kaldi/egs/wsj/s5/steps
   â””â”€â”€ utils -> /myhome/kaldi/egs/wsj/s5/utils
   ```

### prepare_dict.sh

åœ¨prepare_dict.shä¸­å‡†å¤‡æˆ‘ä»¬æ­¤æ¬¡çš„è¯å…¸ã€‚

é€šè¿‡è¿™éƒ¨åˆ†ä»£ç ï¼Œæˆ‘ä»¬æœŸå¾…åœ¨data/dictä¸‹è·å¾—ç»è¿‡å»é‡å’Œè¡¥å……å™ªéŸ³<NOISE>ã€äººå£°å™ªå£°<SPOKEN_NOISE>ã€æœªçŸ¥è¯<UNK>ç­‰çš„è¯å…¸lexicon.txtï¼Œæ’åºå¹¶ç”¨æ•°å­—ç¼–å·çš„å£°å­¦å•å…ƒunits.txtï¼Œä»¥åŠç”¨æ•°å­—æ ‡å·çš„è¯å…¸ï¼Œlexicon_numbers.txtã€‚

å£°å­¦å•å…ƒçš„é€‰æ‹©æœ‰å¤šç§ï¼Œå¯ä»¥æ˜¯éŸ³ç´ phoneã€è‹±æ–‡å­—æ¯characterã€æ±‰å­—ã€ç‰‡æ®µwordpieceç­‰ã€‚è¯å…¸ï¼ˆlexiconï¼‰çš„ä½œç”¨æ˜¯ï¼Œå°†å¾…è¯†åˆ«çš„è¯æ±‡è¡¨ï¼ˆvocabularyï¼‰ä¸­çš„è¯åˆ†è§£ä¸ºå£°å­¦å•å…ƒçš„åºåˆ—ã€‚

1. ç”±äºæˆ‘ä»¬yesnoå®éªŒæ‰€éœ€è¯å…¸è¾ƒå°ï¼Œè¯å…¸ä¿å­˜åœ¨input/lexicon.txtä¸­

   ```
   <SIL> SIL #é™éŸ³silence
   YES Y
   NO N
   ```

2. ç¼–å†™local/prepare_dict.sh

   ```shell
   #!/bin/bash
   
   # This script prepares the phoneme-based lexicon. It also generates the list of lexicon units
   # and represents the lexicon using the indices of the units. 
   
   dir=${H}/data/dict
   mkdir -p $dir
   srcdict=input/lexicon.txt
   
   . ./path.sh
   
   # Check if lexicon dictionary exists
   [ ! -f "$srcdict" ] && echo "No such file $srcdict" && exit 1;
   
   # Raw dictionary preparation
   # grep removes SIL, perl removes repeated lexicons
   cat $srcdict | grep -v "SIL" | \
     perl -e 'while(<>){@A = split; if(! $seen{$A[0]}) {$seen{$A[0]} = 1; print $_;}}' \
     > $dir/lexicon_raw.txt || exit 1;
   
   # Get the set of units in the lexicon without noises
   # cut: remove words, tr: remove spaces and lines, sort -u: sort and unique
   cut -d ' ' -f 2- $dir/lexicon_raw.txt | tr ' ' '\n' | sort -u > $dir/units_raw.txt
   
   # add noises for lexicons
   (echo '<SPOKEN_NOISE> <SPN>'; echo '<UNK> <SPN>'; echo '<NOISE> <NSN>'; ) | \
    cat - $dir/lexicon_raw.txt | sort | uniq > $dir/lexicon.txt || exit 1;
   
   # add noises and number the units
   (echo '<NSN>'; echo '<SPN>';) | cat - $dir/units_raw.txt | awk '{print $1 " " NR}' > $dir/units.txt
   
   # Convert phoneme sequences into the corresponding sequences of units indices, encoded by units.txt
   utils/sym2int.pl -f 2- $dir/units.txt < $dir/lexicon.txt > $dir/lexicon_numbers.txt
   
   echo "Phoneme-based dictionary preparation succeeded"
   ```

   é€šè¿‡è¿™ä¸€è„šæœ¬çš„è¿è¡Œåï¼Œdataç›®å½•ä¸‹ä¼šç”Ÿæˆä¸€ä¸ªdictç›®å½•å¦‚ä¸‹ï¼š

   ```
   â”œâ”€â”€ dict
   â”‚   â”œâ”€â”€ lexicon_raw.txt #åŸè¯å…¸å»é‡å’Œå»éè¯­è¨€å­¦å‘éŸ³
   â”‚   â”œâ”€â”€ units_raw.txt #lexicon_rawè¯å…¸ä¸­éŸ³ç´ å»é‡
   â”‚   â”œâ”€â”€ lexicon.txt #lexicon_rawè¯å…¸åŠ å…¥éè¯­è¨€å­¦å‘éŸ³å¹¶æ’åº
   â”‚   â”œâ”€â”€ units.txt #units_rawæ‰€æœ‰éŸ³ç´ æ ‡å·
   â”‚   â””â”€â”€ lexicon_numbers.txt #ç”¨units.txtä»£è¡¨è¯å…¸æ ‡å·
   ```

   ä»¥ä¸‹å±•ç¤ºdictä¸­æ–‡ä»¶çš„éƒ¨åˆ†å†…å®¹ï¼š

   **lexicon_raw.txt**

   [word] [unit1] [unit2] ...
   
   ```
   YES Y
   NO N
   ```

   **units_raw.txt**
   
   [unit]
   
   ```
   N
   Y
   ```
   
   **lexicon.txt**
   
   ```
   <NOISE> <NSN> #è‡ªè®¤å™ªå£°
   <SPOKEN_NOISE> <SPN> #äººå£°å™ªå£°
   <UNK> <SPN> #æœªçŸ¥è¯
   NO N
   YES Y
   ```
   
   **units.txt**
   
   [unit] [unit_number]
   
   ```
   <NSN> 1
   <SPN> 2
   N 3
   Y 4
   ```
   
   **lexicon_numbers.txt**
   
   [word] [unit_number1] [unit_number2] ...

   ```
   <NOISE> 1
   <SPOKEN_NOISE> 2
   <UNK> 2
   NO 3
   YES 4
   ```
   
   yesnoæ•°æ®é›†ä¸Šäººå£°å™ªå£°å’Œè‡ªç„¶å™ªå£°å¯ä»¥å¿½ç•¥ã€‚

### T.fst & L.fst

FSTï¼ˆFinite State Transducers æœ‰é™çŠ¶æ€è½¬æ¢å™¨ï¼‰FSTå¸¸ä¸WFSTï¼ˆWeighted Finite State Transducers åŠ æƒæœ‰é™çŠ¶æ€è½¬æ¢å™¨ï¼‰çš„ç§°å‘¼æ··ç”¨ï¼Œä¸ä¹‹å·®å¼‚çš„æ˜¯WFSTåœ¨è½¬ç§»è·¯å¾„ä¸Šé™„åŠ äº†æƒé‡ã€‚å®‰è£…openfstæ­£æ˜¯ä¸ºäº†ä½¿ç”¨(W)FSTã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç†è®ºä¸Šï¼Œä¸€ä¸ªWFSTè¡¨ç¤ºäº†è¾“å…¥ç¬¦å·åºåˆ—å’Œè¾“å‡ºç¬¦å·åºåˆ—çš„åŠ æƒå…³ç³»ã€‚

![WFST](assets/WFST.png)


æƒ³è¦äº†è§£æ›´å¤šäº†è§£ä»¥ä¸‹æ–‡çŒ®ï¼š

[M. Mohri, F. Pereira, and M. Riley, â€œSpeech Recognition with Weighted Finite-State Transducersâ€, Handbook on Speech Processing and Speech, Springer, 2008.](https://cs.nyu.edu/~mohri/pub/hbka.pdf)

æ ¹æ®å‘éŸ³è¯å…¸ã€CTCéœ€è¦çš„tokenï¼Œæˆ‘ä»¬ç”Ÿæˆè¯å…¸ï¼ˆLexiconï¼‰çš„L.fstä»¥åŠCTCç”Ÿæˆçš„T.fstã€‚æ­¤å¤„ç”¨åˆ°æˆ‘ä»¬åœ¨prepare_dict.shä¸­å‡†å¤‡å¥½çš„lexicon.txt, units.txt, lexicon_numbers.txtè¿™3ä¸ªæ–‡ä»¶å»ç”Ÿæˆã€‚

```shell
# Compile the lexicon and token FSTs
# generate Lexicon FST L.fst according to words.txt, generate Topology FST T.fst according to tokens.txt
ctc-crf/ctc_compile_dict_token.sh --dict-type "phn" \
  data/dict data/local/lang_phn_tmp data/lang || exit 1;
```

è¯¦è§ctc-crf/ctc_compile_dict_token.shçš„æ³¨é‡Šã€‚

***fstæ–‡ä»¶çš„å¯è§†åŒ–ï¼Œå‚è€ƒ[https://www.cnblogs.com/welen/p/7611320.html]ã€[https://www.dazhuanlan.com/shitou103/topics/1489883]***

é€šè¿‡è¿™ä¸€æ­¥ï¼Œè„šæœ¬ä¾æ¬¡é€šè¿‡lexicon_numbers.txt, units.txtç”Ÿæˆäº†words.txt, tokens.txtï¼Œè¿›è€Œç”Ÿæˆäº†T.fst, L.fstã€‚

**words.txt** ï¼ˆä»£è¡¨äº†L.fstçš„output symbol inventoryï¼Œä¹Ÿå°±æ˜¯G.fstçš„input symbol inventoryï¼‰

```
<eps> 0 #epsilonï¼Œç©ºæ ‡ç­¾ï¼Œè·³å‡ºæ ‡ç­¾ä¸ºç©º
<NOISE> 1
<SPOKEN_NOISE> 2
<UNK> 3
NO 4
YES 5
#0 6 #è¯­è¨€æ¨¡å‹Gçš„å›é€€ç¬¦ï¼Œç¡®å®šG.fst
<s> 7 #èµ·å§‹
</s> 8 #ç»“æŸ
```

FSTç¡®å®šåŒ–ï¼ˆdeterminizationï¼‰æ˜¯æŒ‡ï¼Œå¯¹äºä¸€ä¸ªfstå›¾ï¼Œä»»æ„è¾“å…¥åºåˆ—åªå¯¹åº”å”¯ä¸€è·³è½¬ã€‚æ¶ˆæ­§ç¬¦å·å¸®åŠ©æˆ‘ä»¬ç¡®ä¿æˆ‘ä»¬ä½¿ç”¨çš„WFSTæ˜¯ç¡®å®šåŒ–çš„ï¼Œè¿›ä¸€æ­¥äº†è§£æ¨èé˜…è¯»ã€ŠKaldiè¯­éŸ³è¯†åˆ«å®æˆ˜ã€‹ï¼ˆä½œè€…ï¼šé™ˆæœæœç­‰ï¼‰ç¬¬äº”ç« ã€‚

**tokens.txt** ï¼ˆè®°å½•äº†L.fstçš„input symbol inventoryï¼Œä¹Ÿæ˜¯T.fstçš„output symbol inventoryï¼‰

```
<eps> 0
<blk> 1
<NSN> 2
<SPN> 3
N 4
Y 5
#0 6 #G.fstå›é€€ç¬¦
#1 7 #æ³¨ï¼š#1,#2ä¸ºå¯¹<SPOKEN_NOISE>å’Œ<UNK>çš„æ¶ˆæ­§ï¼Œå› ä¸ºä¸¤è€…éƒ½æ˜ å°„åˆ°<SPN>
#2 8
#3 9 #silçš„æ¶ˆæ­§
```

ä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œä»¥ä¸‹é€šè¿‡fstprintå±•ç¤ºæˆ‘ä»¬ç”Ÿæˆçš„fstæ–‡ä»¶ï¼š

**T.fst**

![T.fst](assets/T.fst.png)


**L.fst**ï¼ˆæ³¨ï¼šå¦‚æœL.fstä¸­æ²¡æœ‰#3çš„è¯ï¼Œåˆ™T.fstä¸­#3ä¹Ÿæ²¡æœ‰å¿…è¦ã€‚å†å²ä¸Šè‹¥ä½¿ç”¨HMMæ‹“æ‰‘ï¼Œåˆ™éœ€è¦å¼•å…¥SIL unitï¼Œæ¯ä¸ªè¯æ±‡å¯æ¥SILä¹Ÿå¯ä»¥ä¸æ¥ï¼Œå› è€ŒL.fstéœ€è¦#3è¿›è¡Œæ¶ˆå²ã€‚æœ¬ä¾‹ä½¿ç”¨CTCæ‹“æ‰‘ï¼ŒL.fstä¸ç”¨#3ï¼‰

![L.fst](assets/L.fst.png)


ä¸ºæ–¹ä¾¿è§‚å¯Ÿï¼Œæˆ‘ä»¬å»æ‰\<NOISE\>,  \<SPOKEN_NOISE\>å±•ç¤ºfstç”Ÿæˆå›¾ï¼Œå½“å‰ï¼š

**words.txt**

```
<eps> 0
NO 1
YES 2
#0 3
<s> 4
</s> 5
```

**tokens.txt**

```
<eps> 0
<blk> 1
N 2
Y 3
#0 4
#1 5
```

**T.fst**

![T.fst(new)](https://github.com/HPLQAQ/CAT/blob/master/assets/T.fst(no%20NOISE).png)


**L.fst**

![L.fst(new)](https://github.com/HPLQAQ/CAT/blob/master/assets/L.fst(no%20NOISE).png)


### G.fst

æ ¹æ®data/train/textã€dict/lexicon.txtï¼Œç”Ÿæˆç”Ÿæˆè¯­è¨€æ¨¡å‹G.fstã€‚

è¿™éƒ¨åˆ†è®­ç»ƒæˆ‘ä»¬é€šè¿‡srilmå·¥å…·å®Œæˆï¼Œæ”¾åˆ°local/yesno_train_lms.shä¸­ã€‚

```shell
# Train and compile LMs. Generate G.fst according to lm, and compose FSTs into TLG.fst
    local/yesno_train_lms.sh data/train/text data/dict/lexicon.txt data/lm || exit 1;
```

**yesno_train_lms.sh**

```shell
#!/bin/bash

# To be run from one directory above this script.

. ./path.sh

text=$1
lexicon=$2
dir=$3
for f in "$text" "$lexicon"; do
  [ ! -f $x ] && echo "$0: No such file $f" && exit 1;
done

#text=data/train/text
#lexicon=data/dict/lexicon.txt
#dir=data/lm
mkdir -p $dir

cleantext=$dir/text.no_oov

# Replace unknown words in text by <UNK>
cat $text | awk -v lex=$lexicon 'BEGIN{while((getline<lex) >0){ seen[$1]=1; } } 
  {for(n=1; n<=NF;n++) {  if (seen[$n]) { printf("%s ", $n); } else {printf("<UNK> ");} } printf("\n");}' \
  > $cleantext || exit 1;

# Count unique words
cat $cleantext | awk '{for(n=2;n<=NF;n++) print $n; }' | sort | uniq -c | \
   sort -nr > $dir/word.counts || exit 1;

# Get counts from acoustic training transcripts, and add  one-count
# for each word in the lexicon (but not silence, we don't want it
# in the LM-- we'll add it optionally later).
cat $cleantext | awk '{for(n=2;n<=NF;n++) print $n; }' | \
  cat - <(grep -w -v '!SIL' $lexicon | awk '{print $1}') | \
   sort | uniq -c | sort -nr > $dir/unigram.counts || exit 1;

# note: we probably won't really make use of <UNK> as there aren't any OOVs
cat $dir/unigram.counts  | awk '{print $2}' | ${H}/local/get_word_map.pl "<s>" "</s>" "<UNK>" > $dir/word_map \
   || exit 1;

# note: ignore 1st field of train.txt, it's the utterance-id.
cat $cleantext | awk -v wmap=$dir/word_map 'BEGIN{while((getline<wmap)>0)map[$1]=$2;}
  { for(n=2;n<=NF;n++) { printf map[$n]; if(n<NF){ printf " "; } else { print ""; }}}' | gzip -c >$dir/train.gz \
   || exit 1;

# LM is small enough that we don't need to prune it (only about 0.7M N-grams).

# From here is some commands to do a baseline with SRILM (assuming
# you have it installed).
heldout_sent=3 
sdir=$dir/srilm
mkdir -p $sdir
cat $cleantext | awk '{for(n=2;n<=NF;n++){ printf $n; if(n<NF) printf " "; else print ""; }}' | \
  head -$heldout_sent > $sdir/heldout
cat $cleantext | awk '{for(n=2;n<=NF;n++){ printf $n; if(n<NF) printf " "; else print ""; }}' | \
  tail -n +$heldout_sent > $sdir/train

cat $dir/word_map | awk '{print $1}' | cat - <(echo "<s>"; echo "</s>" ) > $sdir/wordlist

ngram-count -text $sdir/train -order 1 -limit-vocab -vocab $sdir/wordlist -unk \
  -map-unk "<UNK>" -interpolate -lm $sdir/srilm.o1g.kn.gz
# -kndiscount
ngram -lm $sdir/srilm.o1g.kn.gz -ppl $sdir/heldout 
```

å–3å¥è®¡ç®—å›°æƒ‘åº¦ï¼Œè¿è¡Œç»“æœå¦‚ä¸‹ï¼š

```
file data/lm/srilm/heldout: 3 sentences, 24 words, 0 OOVs
0 zeroprobs, logprob= -11.09502 ppl= 2.575885 ppl1= 2.899294
```

srilmå·¥å…·çš„ä½¿ç”¨å¯ä»¥è§å·¥å…·ä¸‹çš„READMEï¼Œè®­ç»ƒä¸­éœ€è¦å¤„ç†çš„æ–‡ä»¶å­˜æ”¾åœ¨data/lmç›®å½•ä¸‹ï¼Œæˆ‘ä»¬å°†srilmçš„è®­ç»ƒç»“æœå­˜å‚¨åœ¨data/lm/srilmä¸‹ã€‚yesnoå®éªŒä½¿ç”¨1-gramçš„è¯­è¨€æ¨¡å‹çš„ç»“æœï¼Œå‚¨å­˜åˆ°srilm.o1g.knä¸­ï¼Œè¯­è¨€æ¨¡å‹å¦‚ä¸‹ï¼š

**srilm.o1g.km**

```
\data\
ngram 1=7

\1-grams:
-0.9542425	</s>
-99	<NOISE>
-99	<SPOKEN_NOISE>
-99	<UNK>
-99	<s>
-0.3079789	NO
-0.4014005	YES

\end\
```

ä½¿ç”¨n-gramä½œä¸ºè¯­è¨€æ¨¡å‹æ—¶ï¼Œä¹ æƒ¯ä¸Šç”¨ä»¥ä¸Šçš„arpaæ ¼å¼è¡¨ç¤ºï¼Œä»¥ä¸Š[value] [word]çš„å½¢å¼æ„ä¹‰ä¸ºlogP(word)=valueï¼Œç”»å›¾å¦‚ä¸‹ï¼š

**G.fst**

![G.fst](assets/G.fst.png)


### TLG.fst

æŠŠä»¥ä¸Šç”Ÿæˆçš„fstæ–‡ä»¶è¿›è¡Œé‡ç»„ï¼Œç”ŸæˆTLG.fstã€‚

```shell
local/yesno_decode_graph.sh data/lm/srilm/srilm.o1g.kn.gz data/lang data/lang_test || exit 1;
```

è¿™éƒ¨åˆ†ä»£ç ä¸­ï¼Œæˆ‘ä»¬å…ˆå°†è¯­è¨€æ¨¡å‹æ ¹æ®word.txtæ‰“åŒ…åˆ°G.fstä¸­ï¼Œç„¶åç”¨openfstç»„åˆå‡ºTLG.fstï¼Œç”¨äºè®­ç»ƒã€‚

**yesno_decode_graph.sh**

```shell
#!/bin/bash 
#

if [ -f path.sh ]; then . path.sh; fi

#lm_dir=$1
arpa_lm=$1
src_lang=$2
tgt_lang=$3

#arpa_lm=${lm_dir}/3gram-mincount/lm_unpruned.gz
[ ! -f $arpa_lm ] && echo No such file $arpa_lm && exit 1;

rm -rf $tgt_lang
cp -r $src_lang $tgt_lang

# Compose the language model to FST
gunzip -c "$arpa_lm" | \
   grep -v '<s> <s>' | \
   grep -v '</s> <s>' | \
   grep -v '</s> </s>' | \
   arpa2fst - | fstprint | \
   utils/remove_oovs.pl /dev/null | \
   utils/eps2disambig.pl | utils/s2eps.pl | fstcompile --isymbols=$tgt_lang/words.txt \
     --osymbols=$tgt_lang/words.txt  --keep_isymbols=false --keep_osymbols=false | \
    fstrmepsilon | fstarcsort --sort_type=ilabel > $tgt_lang/G.fst


echo  "Checking how stochastic G is (the first of these numbers should be small):"
fstisstochastic $tgt_lang/G.fst 

# Compose the token, lexicon and language-model FST into the final decoding graph
fsttablecompose $tgt_lang/L.fst $tgt_lang/G.fst | fstdeterminizestar --use-log=true | \
    fstminimizeencoded | fstarcsort --sort_type=ilabel > $tgt_lang/LG.fst || exit 1;
fsttablecompose $tgt_lang/T.fst $tgt_lang/LG.fst > $tgt_lang/TLG.fst || exit 1;

echo "Composing decoding graph TLG.fst succeeded"
rm -r $tgt_lang/LG.fst   # We don't need to keep this intermediate FST
```

åˆ°æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†æ•°æ®æ–‡ä»¶çš„å‡†å¤‡ä»¥åŠTLG.fstçš„ç”Ÿæˆï¼ŒTLG.fstç”»å›¾å¦‚ä¸‹ï¼š

![TLG.fst](assets/TLG.png)


	

ç°åœ¨ä½ çš„dataç›®å½•ç»“æ„åº”è¯¥å¦‚ä¸‹ï¼š

```
â”œâ”€â”€ dev
â”‚Â Â  â”œâ”€â”€ spk2utt
â”‚Â Â  â”œâ”€â”€ text
â”‚Â Â  â”œâ”€â”€ utt2spk
â”‚Â Â  â””â”€â”€ wav.scp
â”œâ”€â”€ test
â”‚   ...
â”œâ”€â”€ train
â”‚   ...
â”œâ”€â”€ dict
â”‚Â Â  â”œâ”€â”€ lexicon_numbers.txt
â”‚Â Â  â”œâ”€â”€ lexicon_raw.txt
â”‚Â Â  â”œâ”€â”€ lexicon.txt
â”‚Â Â  â”œâ”€â”€ units_raw.txt
â”‚Â Â  â””â”€â”€ units.txt
â”œâ”€â”€ lang
â”‚Â Â  â”œâ”€â”€ lexicon_numbers.txt
â”‚Â Â  â”œâ”€â”€ L.fst
â”‚Â Â  â”œâ”€â”€ T.fst
â”‚Â Â  â”œâ”€â”€ tokens.txt
â”‚Â Â  â”œâ”€â”€ units.txt
â”‚Â Â  â””â”€â”€ words.txt
â”œâ”€â”€ lang_test
â”‚Â Â  â”œâ”€â”€ G.fst
â”‚Â Â  â”œâ”€â”€ lexicon_numbers.txt
â”‚Â Â  â”œâ”€â”€ L.fst
â”‚Â Â  â”œâ”€â”€ T.fst
â”‚Â Â  â”œâ”€â”€ TLG.fst
â”‚Â Â  â”œâ”€â”€ tokens.txt
â”‚Â Â  â”œâ”€â”€ units.txt
â”‚Â Â  â””â”€â”€ words.txt
â”œâ”€â”€ lm
â”‚Â Â  â”œâ”€â”€ srilm
â”‚Â Â  â”œâ”€â”€ text.no_oov
â”‚Â Â  â”œâ”€â”€ train.gz
â”‚Â Â  â”œâ”€â”€ unigram.counts
â”‚Â Â  â”œâ”€â”€ word.counts
â”‚Â Â  â””â”€â”€ word_map
â”œâ”€â”€ local
â”‚Â Â  â”œâ”€â”€ dev.txt
â”‚Â Â  â”œâ”€â”€ dev_wav.scp
â”‚Â Â  â”œâ”€â”€ lang_phn_tmp
â”‚Â Â  â”œâ”€â”€ test.txt
â”‚Â Â  â”œâ”€â”€ test_wav.scp
â”‚Â Â  â”œâ”€â”€ train.txt
â”‚Â Â  â”œâ”€â”€ train_wav.scp
â”‚Â Â  â”œâ”€â”€ waves_all.list
â”‚Â Â  â”œâ”€â”€ waves.dev
â”‚Â Â  â””â”€â”€ waves.train
â””â”€â”€ waves_yesno
```

è‡³æ­¤æˆ‘ä»¬å·²ç»å®Œæˆyesnoé¡¹ç›®æ­å»ºçš„90%ï¼Œå†æ¬¡ç¡®è®¤ç›®å½•ä¸‹æ¯ä¸ªæ–‡ä»¶ä»£è¡¨å†…å®¹ã€‚

å…³äºè¯å…¸æ–‡ä»¶çš„è¯´æ˜è¾ƒä¸ºç®€ç•¥ï¼Œå¸Œæœ›è¿›ä¸€æ­¥äº†è§£æ¯ä¸€ä¸ªæ–‡ä»¶çš„æ„ä¹‰ï¼Œè¯·é˜…è¯»[Kaldi Data preparation](https://kaldi-asr.org/doc/data_prep.html)æ–‡æ¡£ã€‚

## 2. æå–FBankç‰¹å¾

Step 2: [Feature extraction](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md#Feature-extraction)

ç¬¬äºŒæ­¥ï¼Œæˆ‘ä»¬æå–æ³¢å½¢æ–‡ä»¶çš„FBankç‰¹å¾ï¼ˆFBankæ˜¯Filter Bankçš„ç¼©å†™ï¼ŒæŒ‡éŸ³é¢‘ä¿¡å·ç»è¿‡çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼Œå¾—åˆ°å¹…åº¦è°±ï¼Œå†ç»è¿‡ä¸€ç»„æ»¤æ³¢å™¨ç»„çš„è¾“å‡ºï¼‰ï¼Œæå–çš„FBankç‰¹å¾å­˜æ”¾åœ¨fbankæ–‡ä»¶å¤¹ã€‚

æ³¨æ„åœ¨confç›®å½•ä¸‹å»ºç«‹fbank.confæ–‡ä»¶ï¼Œå†…å®¹ä¸ºï¼š

```
--sample-frequency=8000
--num-mel-bins=40
```

åˆ†åˆ«ä¸ºéŸ³é¢‘é‡‡æ ·ç‡å’Œæ»¤æ³¢å™¨ä¸ªæ•°ï¼Œyesnoæ•°æ®é›†éŸ³é¢‘é‡‡æ ·ç‡ä¸º8000ï¼Œæ»¤æ³¢å™¨ä¸ªæ•°æˆ‘ä»¬å–40ã€‚

***å…³äºFBankï¼š[https://www.jianshu.com/p/b25abb28b6f8]***

```shell
if [ $stage -le 2 ] && [ $stop_stage -ge 2 ]; then
  echo "stage 2: FBank Feature Generation"
  #perturb the speaking speed to achieve data augmentation
  utils/data/perturb_data_dir_speed_3way.sh data/train data/train_sp
  utils/data/perturb_data_dir_speed_3way.sh data/dev data/dev_sp
  
  # Generate the fbank features; by default 40-dimensional fbanks on each frame
  fbankdir=fbank
  for set in train_sp dev_sp; do
    steps/make_fbank.sh --cmd "$train_cmd" --nj 1 data/$set exp/make_fbank/$set $fbankdir || exit 1;
    utils/fix_data_dir.sh data/$set || exit;  #filter and sort the data files
    steps/compute_cmvn_stats.sh data/$set exp/make_fbank/$set $fbankdir || exit 1;  #achieve cmvn normalization
  done

  for set in test; do
    steps/make_fbank.sh --cmd "$train_cmd" --nj 1 data/$set exp/make_fbank/$set $fbankdir || exit 1;
    utils/fix_data_dir.sh data/$set || exit;  #filter and sort the data files
    steps/compute_cmvn_stats.sh data/$set exp/make_fbank/$set $fbankdir || exit 1;  #achieve cmvn normalization
  done
fi
```

åœ¨æå–å£°éŸ³æ–‡ä»¶çš„ç‰¹å¾æ—¶ï¼Œæ­¤å¤„ä½¿ç”¨äº†å°†å£°éŸ³è¿›è¡Œ0.9ã€1.0ã€1.1ä¸‰ç§å˜é€Ÿçš„æ“ä½œï¼Œåœ¨ä¸€èˆ¬è¯†åˆ«ä»»åŠ¡ä¸­æ•ˆæœä¼šæ›´å¥½ã€‚yesnoé¡¹ç›®æˆ‘ä»¬ä¸åšæ­¤æ“ä½œï¼Œæ­¤å¤„ä½¿ç”¨è¯¥ä»£ç ä½œä¸ºæ¼”ç¤ºã€‚ç›¸å…³è„šæœ¬ç®€è¦è¯´æ˜å¦‚ä¸‹ï¼š

* utils/data/perturb_data_dir_speed_3way.shï¼šå˜é€Ÿè„šæœ¬

* steps/make_fbank.shï¼šfbankæå–è„šæœ¬

* utils/fix_data_dir.shï¼šæ•°æ®æ’åºä¸è¿‡æ»¤

* steps/compute_cmvn_stats.shï¼šç‰¹å¾å½’ä¸€åŒ–ï¼Œcmvnæ˜¯æŒ‡cepstra mean and variance normalizationï¼Œå³å‡å»å‡å€¼é™¤ä»¥æ ‡å‡†å·®çš„æ“ä½œã€‚æ—©æœŸè¯­éŸ³è¯†åˆ«ä¸­æå–çš„éŸ³é¢‘ç‰¹å¾æ˜¯å€’è°±ï¼Œæ•…ç”±æ­¤å¾—åã€‚åœ¨FBankç‰¹å¾å¾—å½’ä¸€åŒ–å¤„ç†ï¼Œä¹Ÿæ²¿ç”¨äº†è¯¥ç§°å‘¼ã€‚

## 3. å‡†å¤‡åˆ†æ¯å›¾è¯­è¨€æ¨¡å‹

Step 3: [Denominator LM preparation](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md#Denominator-LM-preparation)

åœ¨ç¬¬3æ­¥ï¼Œæˆ‘ä»¬å…ˆå¾—åˆ°å¾—åˆ°è®­ç»ƒé›†ä¸­æ¯å¥è¯çš„æ ‡ç­¾ï¼ˆlabelï¼‰åºåˆ—ï¼Œå¯èƒ½ç”¨åˆ°çš„æ ‡ç­¾é›†ï¼ˆlabel inventoryï¼‰ä¿å­˜åœ¨units.txtä¸­ã€‚ç„¶åï¼Œé€šè¿‡è®¡ç®—æ ‡ç­¾åºåˆ—çš„è¯­è¨€æ¨¡å‹å¹¶å°†å…¶è¡¨ç¤ºæˆden_lm.fstã€‚æœ€åï¼Œç”±den_lm.fstå’Œæ ‡ç­¾æ–‡ä»¶å‡ºå‘ï¼Œè®¡ç®—å‡ºæ ‡ç­¾åºåˆ—$l$çš„å¯¹æ•°æ¦‚ç‡ $logp(l)$ï¼Œç§°ä¸ºpath weightã€‚è¯¦ç»†çš„æ­¥éª¤å†…å®¹è§æ³¨é‡Šã€‚

```shell
data_tr=data/train_sp
data_cv=data/dev_sp

if [ $stage -le 3 ] && [ $stop_stage -ge 3 ]; then
  #convert word sequences to label sequences according to lexicon_numbers.txt and text files in data/lang_phn
  #the result will be placed in $data_tr/ and $data_cv/
  ctc-crf/prep_ctc_trans.py data/lang/lexicon_numbers.txt $data_tr/text "<UNK>" > $data_tr/text_number
  ctc-crf/prep_ctc_trans.py data/lang/lexicon_numbers.txt $data_cv/text "<UNK>" > $data_cv/text_number
  echo "convert text_number finished"

  # prepare denominator
  ctc-crf/prep_ctc_trans.py data/lang/lexicon_numbers.txt data/train/text "<UNK>" > data/train/text_number
  #sort the text_number file, and then remove the duplicate lines
  cat data/train/text_number | sort -k 2 | uniq -f 1 > data/train/unique_text_number
  mkdir -p data/den_meta
  #generate phone_lm.fst, a phone-based language model
  chain-est-phone-lm ark:data/train/unique_text_number data/den_meta/phone_lm.fst
  #generate the correct T.fst, called T_den.fst
  ctc-crf/ctc_token_fst_corrected.py den data/lang/tokens.txt | fstcompile | fstarcsort --sort_type=olabel > data/den_meta/T_den.fst
  #compose T_den.fst and phone_lm.fst into den_lm.fst
  fstcompose data/den_meta/T_den.fst data/den_meta/phone_lm.fst > data/den_meta/den_lm.fst
  echo "prepare denominator finished"
  
  #calculate and save the weight for each label sequence based on text_number and phone_lm.fst
  path_weight $data_tr/text_number data/den_meta/phone_lm.fst > $data_tr/weight
  path_weight $data_cv/text_number data/den_meta/phone_lm.fst > $data_cv/weight
  echo "prepare weight finished"
fi
```

## 4. ç¥ç»ç½‘ç»œè®­ç»ƒå‡†å¤‡

Step 4: [Neural network training preparation](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md#Neural-network-training-preparation)

ä¸åŒè¯­éŸ³è¯†åˆ«é¡¹ç›®ä¸­ï¼Œè¿™éƒ¨åˆ†å¤„ç†å·®åˆ«ä¸å¤§ã€‚æˆ‘ä»¬å¯¹æ•°æ®é›†çš„çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å¹¶å’Œä¹‹å‰è®¡ç®—çš„path weightsä¸€èµ·æ•´åˆåˆ°data/pickleä¸‹ã€‚

```shell
if [ $stage -le 4 ] && [ $stop_stage -ge 4 ]; then
  mkdir -p data/all_ark
  
  for set in test; do
    eval data_$set=data/$set
  done

  for set in test cv tr; do
    tmp_data=`eval echo '$'data_$set`

    #apply CMVN feature normalization, calculate delta features, then sub-sample the input feature sequence
    feats="ark,s,cs:apply-cmvn --norm-vars=true --utt2spk=ark:$tmp_data/utt2spk scp:$tmp_data/cmvn.scp scp:$tmp_data/feats.scp ark:- \
    | add-deltas ark:- ark:- | subsample-feats --n=3 ark:- ark:- |"

    ark_dir=$(readlink -f data/all_ark)/$set.ark
    #copy feature files, generate scp and ark files to save features.
    copy-feats "$feats" "ark,scp:$ark_dir,data/all_ark/$set.scp" || exit 1
  done
fi

if [ $stage -le 5 ] && [ $stop_stage -ge 5 ]; then
  mkdir -p data/pickle
  #create a pickle file to save the feature, text_number and path weights.
  python3 ctc-crf/convert_to.py -f=pickle --describe='L//4' --filer=1500 \
      data/all_ark/cv.scp $data_cv/text_number $data_cv/weight data/pickle/cv.pickle || exit 1
  python3 ctc-crf/convert_to.py -f=pickle --describe='L//4' --filer=1500 \
      data/all_ark/tr.scp $data_tr/text_number $data_tr/weight data/pickle/tr.pickle || exit 1
fi
```

åœ¨stage 5ç»“æŸåï¼Œç”¨`fi`ç»“æŸæœ€å¼€å§‹```if [ $NODE == 0 ]; then```çš„å¤§æ‹¬å·ï¼Œè¿›å…¥åˆ°è®­ç»ƒéƒ¨åˆ†ã€‚

## 5. æ¨¡å‹è®­ç»ƒ

Step 5: [Model training](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md#Model-training)

æ­¤æ—¶æ¨¡å‹è®­ç»ƒéœ€è¦çš„æ‰€æœ‰æ•°æ®å·²ç»å‡†å¤‡å®Œæˆï¼Œå‰©ä¸‹åªéœ€è¦åœ¨expä¸‹åˆ›å»ºä½ çš„ä¸€æ¬¡å®éªŒçš„æ–‡ä»¶å¤¹(demo)ï¼Œå»ºç«‹config.jsonï¼Œæ­¤å¤„yesnoå®éªŒå¯ä»¥å°†config.jsonè¿›è¡Œä¿®æ”¹å¤šæ¬¡å®éªŒï¼š

```json
{
    "net": {
        "type": "LSTM",
        "lossfn": "crf",
        "lamb": 0.01,
        "kwargs": {
            "n_layers": 3,
            "idim": 120,
            "hdim": 320,
            "num_classes": 5,
            "dropout": 0.5
        }
    },
    "scheduler": {
        "type": "SchedulerCosineAnnealing",
        "optimizer": {
            "type_optim": "Adam",
            "kwargs": {
                "lr": 1e-3,
                "betas": [
                    0.9,
                    0.99
                ],
                "weight_decay": 0.0
            }
        },
        "kwargs": {
            "lr_min": 1e-5,
            "period": 5,
            "epoch_max": 30,
            "reverse_metric_direc": true
        }
    }
}
```

netå‚æ•°è®¾ç½®è®­ç»ƒä½¿ç”¨çš„æ¨¡å‹ï¼Œå‚è€ƒctc_crf/model.pyï¼›schedulerå‚æ•°è®¾ç½®å­¦ä¹ çš„ç­–ç•¥ï¼Œå‚è€ƒctc_crf/scheduler.pyï¼Œoptimizerå‚æ•°è¯·é˜…è¯»[torch.optim][https://pytorch.org/docs/stable/optim.html]ç›¸å…³æ–‡æ¡£ã€‚æ­¤å¤„æˆ‘ä»¬é‡‡ç”¨LSTMæ¨¡å‹ï¼Œå­¦ä¹ ç‡è¡°å‡ä½¿ç”¨ä½™å¼¦é€€ç«ç­–ç•¥ã€‚

è®­ç»ƒçš„ä»£ç å¦‚ä¸‹ï¼š

```shell
PARENTDIR='.'
dir="exp/demo"
DATAPATH=$PARENTDIR/data/

if [ $stage -le 6 ] && [ $stop_stage -ge 6 ]; then

  if [ $change_config == 1 ]; then
    rm $dir/scripts.tar.gz
    rm -rf $dir/ckpt
  fi

  unset CUDA_VISIBLE_DEVICES

  if [[ $NODE == 0 && ! -f $dir/scripts.tar.gz ]]; then
    echo ""
    tar -zcf $dir/scripts.tar.gz $(readlink ctc-crf) $0
  elif [ $NODE == 0 ]; then
    echo ""
    echo "'$dir/scripts.tar.gz' already exists."
    echo "If you want to update it, please manually rm it then re-run this script."
  fi

  # uncomment the following line if you want to use specified GPUs
  CUDA_VISIBLE_DEVICES="0"                      \
  python3 ctc-crf/train.py --seed=0             \
    --world-size 1 --rank $NODE                 \
    --batch_size=3                              \
    --dir=$dir                                  \
    --config=$dir/config.json                   \
    --data=$DATAPATH                            \
    || exit 1
fi
```

é€šè¿‡ä»¥ä¸Šä»£ç å³å¯å®Œæˆæ¨¡å‹è®­ç»ƒã€‚è®­ç»ƒçš„è¿‡ç¨‹å›¾å±•ç¤ºå¯ä»¥åœ¨ä½ åˆ›å»ºçš„demoç›®å½•ä¸‹çš„monitor.jpgä¸­æ‰¾åˆ°ã€‚

å¦‚æœéœ€è¦é‡æ–°è®­ç»ƒï¼Œåˆ é™¤scripts.tar.gzå’Œckptæ–‡ä»¶å³å¯ã€‚

## 6. è§£ç 

Step 6: [Decoding](https://github.com/thu-spmi/CAT/blob/master/toolkitworkflow.md#Decoding)

è®¡ç®—æµ‹è¯•é›†ä¸­æ¯å¥è¯æ¯å¸§çš„logitså¹¶è§£ç ã€‚

```shell
nj=1
if [ $stage -le 7 ] && [ $stop_stage -ge 7 ]; then
  for set in test; do
    ark_dir=$dir/logits/$set
    mkdir -p $ark_dir
    python3 ctc-crf/calculate_logits.py               \
      --resume=$dir/ckpt/bestckpt.pt                     \
      --config=$dir/config.json                       \
      --nj=$nj --input_scp=data/all_ark/$set.scp      \
      --output_dir=$ark_dir                           \
      || exit 1
  done
fi


if [ $stage -le 8 ] && [ $stop_stage -ge 8 ]; then
  for set in test; do
    mkdir -p $dir/decode_${set}
    ln -s $(readlink -f $dir/logits/$set) $dir/decode_${set}/logits
    ctc-crf/decode.sh --stage 1 \
        --cmd "$decode_cmd" --nj 1 --acwt 1.0 --post_decode_acwt 1.0\
        data/lang_${set} data/${set} data/all_ark/${set}.scp $dir/decode_${set}
  done
fi

if [ $stage -le 9 ] && [ $stop_stage -ge 9 ]; then
  for set in test; do
    grep WER $dir/decode_${set}/wer_* | utils/best_wer.sh
  done
fi
```

æ­å–œä½ å·²ç»å®Œæˆäº†ä½ çš„ç¬¬ä¸€ä¸ªyesnoè¯­éŸ³è¯†åˆ«é¡¹ç›®çš„æ­å»ºï¼Œè®­ç»ƒå’Œè§£ç è¿‡ç¨‹ã€‚

ç°åœ¨ä½ çš„ç›®å½•ç»“æ„åº”è¯¥å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

```
â”œâ”€â”€ cmd.sh
â”œâ”€â”€ conf
â”‚Â Â  â”œâ”€â”€ decode_dnn.config
â”‚Â Â  â”œâ”€â”€ fbank.conf
â”‚Â Â  â””â”€â”€ mfcc.conf
â”œâ”€â”€ ctc-crf -> ../../scripts/ctc-crf
â”œâ”€â”€ exp
â”‚Â Â  â””â”€â”€ demo
â”œâ”€â”€ input
â”‚Â Â  â””â”€â”€ lexicon.txt
â”œâ”€â”€ local
â”‚Â Â  â”œâ”€â”€ create_yesno_txt.pl
â”‚Â Â  â”œâ”€â”€ create_yesno_waves_test_train.pl
â”‚Â Â  â”œâ”€â”€ create_yesno_wav_scp.pl
â”‚Â Â  â”œâ”€â”€ get_word_map.pl
â”‚Â Â  â”œâ”€â”€ prepare_data.sh
â”‚Â Â  â”œâ”€â”€ prepare_dict.sh
â”‚Â Â  â”œâ”€â”€ score.sh
â”‚Â Â  â”œâ”€â”€ yesno_decode_graph.sh
â”‚Â Â  â””â”€â”€ yesno_train_lms.sh
â”œâ”€â”€ path.sh
â”œâ”€â”€ run.sh
â”œâ”€â”€ steps -> /myhome/kaldi/egs/wsj/s5/steps
â””â”€â”€ utils -> /myhome/kaldi/egs/wsj/s5/utils
```
## 7. å®éªŒå±•ç¤º
	
ä»¥ä¸‹æ˜¯ä¸€æ¬¡é»˜è®¤è®­ç»ƒç»“æœå±•ç¤ºï¼š
	
![monitor](https://user-images.githubusercontent.com/99643269/158049088-f21ea54a-66be-43cd-801b-ad05cea6e2b0.png)

è¯†åˆ«ç»“æœå¦‚ä¸‹ï¼š
	
```
%WER 5.83 [ 14 / 240, 1 ins, 13 del, 0 sub ] 
```

è¯†åˆ«çš„è¯¦ç»†logåœ¨exp/demo/decode_testä¸­ã€‚

åœ¨è®­ç»ƒå®Œæˆåï¼Œè¯·åœ¨demoæ–‡ä»¶å¤¹ä¸‹è‡ªåŠ¨ç”Ÿæˆçš„readme.mdæ–‡ä»¶ä¸­å¯¹ä½ çš„è¿™æ¬¡å®éªŒè¿›è¡Œè®°å½•ã€‚

## 8. ç»“æœåˆ†æ
	
**æˆ‘ä»¬åˆ©ç”¨`waves_yesno`æä¾›çš„60æ¡è¯­éŸ³æ•°æ®æŒ‰ç…§5ï¼š5å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†**	
	
**å®šä¹‰åˆé€‚çš„å­¦ä¹ ç‡ã€è¿­ä»£æ¬¡æ•°ï¼Œæ ¹æ®æ¢¯åº¦ä¸‹é™æ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œåˆ†åˆ«çš„è®­ç»ƒï¼Œå¹¶è®°å½•æ¯ä¸€æ¬¡è®­ç»ƒçš„å¹³å‡æŸå¤±å‡½æ•°å€¼**

ä»¥ä¸‹æ˜¯è¿›è¡Œ8æ¬¡å®éªŒçš„ç»“æœå¯¹æ¯”ï¼š
	
![image](https://user-images.githubusercontent.com/99643269/158049903-7fccdc78-8ef7-4b95-af1a-1f213dd96b15.png)

**å®éªŒç»“æœå¯ä»¥çœ‹å‡ºCAT(ctc-crf)è¦ä¼˜äºctcï¼Œç”±äºyesnoå®éªŒæ•°æ®ç®€å•ç”Ÿæˆçš„è¯­è¨€æ¨¡å‹å¹¶ä¸å¤æ‚æ‰€ä»¥1-gramè¦æ¯”å¤šé˜¶è¯­è¨€æ¨¡å‹æ•ˆæœæ›´å¥½**
	
**ä¹Ÿå¯ä»¥å°è¯•ä¿®æ”¹`exp/demo/config.jsn`ä¸­å‚æ•°å°è¯•å¤šæ¬¡è®­ç»ƒ**

**ğŸ±â€ğŸ**	
